{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is linear in a generalized linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLM typically contrains 3 components\n",
    "1. The probability distribution from exponential family\n",
    "2. The linear predictor\n",
    "3. The link function which connects the mean of 1. to the linear predictor\n",
    "\n",
    "So, the linear part is the linear preditor in GLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between joint and conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Joint probability measures how likely two (or more) things will both occur\n",
    "+ Conditional probability measure how likely one thing happen if you know the other things has happened. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement logistic regression training for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We have m training data points $((x^1, y^1), (x^2, y^2), ... ,(x^m, y^m))$. \n",
    "+ Each x is resent by a n-dimension vector x = $(x_1, x_2, .., x_n)$\n",
    "+ Linear predictor: $p(y=1|x) = y' = f(x)$ where f is the sigmoid function $f = 1/(1 + exp(-w*x))$ and w is the weight vector; $p(y=0|x) = 1-y'$\n",
    "\n",
    "Cross-entropy cost function between predicted value and grouth truth is: $H(y, y') = -y*log(y') - (1-y)log(1-y')$\n",
    "\n",
    "Gradient of the cost function with regard to $i^{th}$ dimension of input vector $x = (y' - y) * x_i$\n",
    "\n",
    "So the loss function over the training data: $L(w) = \\frac{1}{m} \\sum_{j=1,m} {[-y_j*log(y'_j) - (1-y_j)log(1-y'_j)]}$\n",
    "\n",
    "minimize L(w) using gradient descent\n",
    "\n",
    "repeat{    \n",
    "    $w_i = w_i - \\alpha * \\sum_{j=1,m} {(y'_j -y_j)*x_{j,i}}$    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between Naive Bayes and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling:\n",
    "- Naive Bayes: generative model of joint probability $p(x,y)$ and compute $p(y|x)$ via Bayes rule\n",
    "- Logistic regression: discriminative model that directly compute conditional probability $p(y|x)$\n",
    "\n",
    "Training:\n",
    "- Naive Bayes: each feature weight is set independently based on how much the feature correlates with the label\n",
    "- Logistic regression: feature weights are set together\n",
    "- Let say you have features that highly correlated with the label, and the features are correlated themselves. The NB model will double-counted the feature impacts while the LR will try to balance them.\n",
    "- Feature engineering: you can throw in many correlated features in LR but in NB features must be carefully designed.\n",
    "\n",
    "\n",
    "http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between linear regression and logistic regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use linear regression when you want to predict continuous outcome. \n",
    "+ Use logistic regression when the outcome is categorical.\n",
    "\n",
    "For example, given X is the number of square feet in a house. If you want to predict what is the selling price of the house you will use linear regression.\n",
    "On the other hand, if you want to predict whether or not the house would sell the house more than $500K you will use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between generative and discriminative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is maximum likelihood, cost function, gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some alternatives to gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## What is the EM algorithm? Give a couple of applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "+ EM (Expectation-Maximization) algorithm is the generalization of maximum likelihood estimation (MLE) to the incomplete data set.\n",
    "+ Let denote X is observed data, Z is a latent variable representing the unobserved/missing data; $\\theta$ is model parameters with likelihood function $L(\\theta; X, Z) = p(X, Z|\\theta)$\n",
    "+ The MLE of unknown parameters is the marginal likelihood over the observed data $L(\\theta; X) = p(X;\\theta) = \\sum_Z {p(X,Z|\\theta)}$\n",
    "+ EM is iteratively guessing a distribution for the unobserved data, then estimating the model parameters by maximizing something that is a lower bound on the actual likelihood function, and repeating until convergence.\n",
    "\n",
    "+ EM iteratively optimizes parameters with 2 steps\n",
    "    - Expectation (E-step): calculate the expected valued of log likelihood function with respect to the conditional distribution of Z given X under the current parameter $\\theta^t$\n",
    "        + $Q(\\theta|\\theta^t) = E_{Z|X, \\theta^t}[log L(\\theta; X, Z)]$\n",
    "        \n",
    "    - Maximization (M-step): find the parameter that maximizes Q\n",
    "        + $\\theta^t = argmax_\\theta {Q(\\theta|\\theta^t)}$\n",
    "        \n",
    "Notes:\n",
    "    - If underflow occurs, use log-sum trick\n",
    "    - EM does not guarantee to converge to a global maximum. It can ends up with a local maximum, use some tricks to escape such as random restart, simulated annealing \n",
    "    - EM is particularly useful when the likelihood is an exponential family: the E-step becomes the sum of expectations of sufficient statistics; M-step involves maximizing a linear function\n",
    "    \n",
    "Applications:\n",
    "+ GMM: e-step estimate label assigment for each datapoint given the current gmm params; m-step maximize the new param given new labal assigments\n",
    "+ HMM: the Baum-Welch algorithm to find unknown params of HMM; it uses foward-backward algorithm\n",
    "+ Parsing: inside-outside algorithms\n",
    "+ K-means clustering is a special case of EM\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Explain what regularization is and why it is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a probabilistic graphical model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between Markov networks and Bayesian networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undirected versus directed\n",
    "Markov is undirected\n",
    "globally normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain decision tree & decision forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What's gradient boosted trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain kernel tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix factorization/Model selection/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On what type of ensemble technique is a random forest based? What particular limitation does it try to address?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Ax=b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: $x = A^{-1} b$ where A must be invertible (nonsingular) which means \n",
    "+ A is a square matrix nxn\n",
    "+ Exist a square matrix nxn B such that AB=BA=I where I is identity matrix\n",
    "\n",
    "Methods for matrix inversion: depending on matrix A properties we can use Gaussian elimination/LU decomposition, Newton's methods, Eigen decomposition, Cholesky decomposition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give an example of an application of non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What methods for dimensionality reduction do you know and how do they compare with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some good ways for performing feature selection that do not involve exhaustive search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## What does the curse of dimensionality mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The curse of dimensionality means that when the dimensionality increases, the volume of the space increase so fast that the available data become sparse. This sparsity is problematic for ML methods that require statistical significance. In order to obtain reliable result, the amount of data grows exponentially. Following is a simple example.\n",
    "\n",
    "Let's say you have a straight line 100 yards long and you dropped a penny somewhere on it. It wouldn't be too hard to find. You walk along the line and it takes two minutes.\n",
    "\n",
    "Now let's say you have a square 100 yards on each side and you dropped a penny somewhere on it. It would be pretty hard, like searching across two football fields stuck together. It could take days.\n",
    "\n",
    "Now a cube 100 yards across. That's like searching a 30-story building the size of a football stadium. Ugh.\n",
    "\n",
    "The difficulty of searching through the space gets a lot harder as you have more dimensions. You might not realize this intuitively when it's just stated in mathematical formulas, since they all have the same \"width\". That's the curse of dimensionality. It gets to have a name because it is unintuitive, useful, and yet simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you evaluate the quality of the clusters that are generated by a run of K-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain A/B testing and give a concrete example how to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you were training a classifier, which metrics would you use for model selection and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it better to have too many false positives, or too many false negatives? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is selection bias, why is it important and how can you avoid it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is root cause analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain price optimization, price elasticity, inventory management, competitive intelligence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain what resampling methods are and why they are useful. Also explain their limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some of the main characteristics that distinguish deep learning from traditional machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why a NN need activation functions? Compare & contrast some activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Differences between classification error/mean square error and cross entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we are training a NN that predicts catergorical values (e.g. digit classification) cross entropy error (CE) is generally better than mean square error (MSE).\n",
    "\n",
    "Let look at a example where we want to predict the U.S. presidential outcome given three training data point.\n",
    "\n",
    "The 1st NN output\n",
    "\n",
    "|computed       | grouth truth         | correct?|\n",
    "|------------------------------------------------|\n",
    "|0.3  0.3  0.4  | 0  0  1 (democrat)   | yes     |\n",
    "|0.3  0.4  0.3  | 0  1  0 (republican) | yes     |\n",
    "|0.1  0.2  0.7  | 1  0  0 (other)      | no      |\n",
    "\n",
    "\n",
    "The 2nd NN output\n",
    "\n",
    "|computed       | grouth truth         | correct?|\n",
    "|------------------------------------------------|\n",
    "|0.1  0.2  0.7  | 0  0  1 (democrat)   | yes     |\n",
    "|0.1  0.7  0.1  | 0  1  0 (republican) | yes     |\n",
    "|0.3  0.4  0.3  | 1  0  0 (other)      | no      |\n",
    "\n",
    "Both NNs has classification error of 1/3 = 0.33. However, the 1st NN barely gets the first two training examples correct and way off on the third one. Meanwhile, the 2nd NN is better than the 1st NN because the first two training examples are strongly correct.\n",
    "\n",
    "So classification error is very crude to use for optimizing the NN. \n",
    "\n",
    "Now consider the cross-entropy error. The average cross-entropy error (ACE) over the training example is typically used in the NN training.\n",
    "\n",
    "ACE of the 1st NN = $1/3 * [-[0*log(0.3) + 0*log(0.3) + 1*log(0.4)] - [0*log(0.3) + 1*log(0.4) + 0*log(0.3)] - [1*log(0.1) + 0*log(0.2) + 0*log(0.7)]]$\n",
    "                  = $-1/3 * [log(0.4) + log(0.4) + log(0.1)] = 1.38$\n",
    "\n",
    "ACE of the 2nd NN = -1/3 * [log(0.7) + log(0.7) + log(0.3)] = 0.64\n",
    "\n",
    "The ACE of the 2nd NN is much smaller than the 1st NN which indicates the 2nd NN is better than the 1st one.\n",
    "\n",
    "Let consider the mean square error (MSE)\n",
    "\n",
    "MSE of the 1st NN = $1/3 * [ [(0.3-0)^2 + (0.3-0)^2 + (0.4-1)^2] + [(0.3-0)^2 + (0.4-1)^2 + (0.3-0)^2] + [(0.1-1)^2 + (0.2-0)^2 + (0.7-0)^2]]$\n",
    "                  = $1/3 * [0.54 + 0.54 + 1.34] = 0.81 $\n",
    "                  \n",
    "MSE of the 2nd NN = $1/3 [0.14 + 0.14 + 0.74] = 0.34 $\n",
    "\n",
    "MSE isn’t a hideously bad approach but if you think about how MSE is computed you’ll see that, compared to ACE, MSE gives too much emphasis to the incorrect outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare & contrast CNN & RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's HMM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's CRF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the relationship between MaxEnt & logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.quora.com/What-is-the-relationship-between-Log-Linear-model-MaxEnt-model-and-Logistic-Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's sequence-to-sequence model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compare & contrast sequence-to-sequence model with other NLP models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html/3    \n",
    "+ https://www.quora.com/What-are-the-best-interview-questions-to-evaluate-a-machine-learning-researcher\n",
    "+ http://stats.stackexchange.com/questions/65379/machine-learning-curse-of-dimensionality-explained\n",
    "+ https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/\n",
    "+ http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf\n",
    "+ http://www.wildml.com/deep-learning-glossary/\n",
    "+ https://www.quora.com/What-is-the-relationship-between-Log-Linear-model-MaxEnt-model-and-Logistic-Regression\n",
    "\n",
    "Coding:\n",
    "+ http://www.interviewkickstart.com/\n",
    "+ https://leetcode.com/problemset/algorithms/\n",
    "+ https://www.interviewcake.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "725px",
   "left": "1351.22px",
   "right": "20px",
   "top": "20px",
   "width": "549px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
